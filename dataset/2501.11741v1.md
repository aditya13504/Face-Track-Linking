FaceSORT: a Multi-Face Tracking Metho d based on Biometric and
App earance Feat ur es
Rob ertochl and Andreas Uhl
University of Salzburg, Department of Articial Intelligence and Human Interfaces,
Salzburg, Austria
f
rob ert.jo echl, andreas.uhl
g
@plus.ac.at
Abstract
Tracking multiple faces is a dicult problem, as there may
b e partially o ccluded or lateral faces. In multiple face track-
ing, asso ciation is typically based on (biometric) face fea-
tures. However, the mo dels used to extract these face fea-
tures usually require frontal face images, w hich can limit the
tracking p erformance. In this work, a multi-face tracking
metho d inspired by StrongSort, Face SORT, is prop osed. To
mitigate the problem of partially o ccluded or lateral faces,
biometric face features are combined with visual app earance
features (i.e., generated by a generic ob ject classier), with
b oth features are extracted from the same face patch. A com-
prehensive exp erimental evaluation is p erformed, including a
comparison of dierent face descriptors, an evaluation of dif-
ferent parameter settings, and the application of a die re nt
similarity metric. All exp eriments are conducted with a new
multi-face tracking dataset and a subset of the ChokePoint
dataset. The `Paris Lo dron University Salzburg Faces in a
Queue' dataset consists of a total of s even fully annotated
sequences (12730 frames) and is made publicly available as
part of this work. Together with this dataset, annotations of
6 sequences from the ChokePoint dataset are also provided.
1 Intro duction
Multiple ob ject tracking (MOT) [1] is ab out nding the tra-
jectories of all ob jects app earing in a sequence. These ob jects
could b e from dierent classes or a single c las s (e.g. p e de s-
trians, cars, p ersons, etc.). When conside ring faces, MOT is
referred to as multi-face tracking. This work fo cuses on track-
ing the faces of p eople moving towards a gate in a queue.
The ass um ed scenario is that a group of p eople is m oving
towards a gate (e.g., to enter a sp orts stadium). As they
move towards a single gate, they form a queue. However,
as this is a leisure activity, this queue will not b e very well
organized (p eople chat with each other, they eat, they move
around in a disorderly fashion, there may b e pushing and
shoving, etc.). In the as sumed scenario, the main ob jective
is to track p eople's faces as they move towards the gate. For
this purp ose, a camera is mounted on the gate. The cam-
era is dire cted at the queue such that the rst p erson visible
is standing directly in front of the gate. However, since the
queue is probably not very we ll organized, the main track-
ing challenge s are o cclusions (partially and full), out-of-plane
rotations (lateral faces) and non-linear motion.
To solve a general MOT problem, basically two paradigms
exist: (i) joint-detection-asso ciation (JDA), and (ii) tracking-
by-detection (TbD). With a JDA based tracker, basically ev-
erything is learned (end-to-end), i.e., the ob ject dete ction,
the relevant cues for the asso ciation and the asso ciation of
ob jects across frames. Rece nt metho ds in this context are
[2, 3, 4, 5, 6]. In this work, however, the fo cus is on TbD
metho ds [7, 8, 9, 10, 11, 12]. The rst step of a tracker
based on the TbD paradigm is the detection of all ob jects
(e.g., faces) in the current frame. The tracking p erform anc e
is therefore directly related to the detection p e rformance . In
[10], a Spatial-Temp oral Top ology-based Detector (STTD) is
prop osed. A feature mixing strategy is prop osed in [13] to
improve the detection p erform anc e in the context of MOT.
Tracking is then ab out nding bipartite matches for all de-
tected ob jects and active tracks (i.e ., asso ciation problem).
A common way to s olve this asso ciation problem is to apply
the Hungarian algorithm [14] to a cost matrix (representing
the costs of matching the
i
-th ac tive track with the
j
-th de-
tected ob ject). These costs can, for exam ple , b e based on mo-
tion cues and/or app earance cues. Prominent examples for
TbD tracker are Simple Online Realtime Tracking (SORT)
[15] (motion based asso ciation), DeepSORT [16] (asso ciated
based on motion and app earance features) and StrongSORT
[17] (improved version of DeepSORT). In [12], a transformer
is used to estimate complex motion cues. A combination
of motion and app earance mo dels in a single network, called
UMA, is prop osed in [18]. In [9], a fusion of six distance met-
rics (for motion and app earance cues) is used for asso ciation.
One advantage of TbD trackers over JDA trackers is that
pre-trained mo dels can b e utilized for the individual subtasks
(e.g., a face detector and fac e rec ognition mo del), while p er-
formance is comp etitive (as demonstrated in [19]). A form of
TbD tracke rs are multi hyp othesis trackers [20, 21]. In multi
hyp othesis tracking, several track hyp othesis are maintained
simultaneously for the tra jectory of each ob ject.
1
arXiv:2501.11741v1  [cs.CV]  20 Jan 2025
In TbD based multi-face trackers, asso ciation is based on
(biometric) face features (i.e., deep neural networks trained
in the context of face re cognition). For example, a two-stage
asso ciation based on motion and face features (ArcFace [22]
trained on the MS-Celeb-1M dataset [23]) is prop osed in [24].
Multi-face tracking based on DeepSORT [16] is prop osed in
[25]. The authors evaluate dierent combinations of face de-
tectors (i.e. MTCNN [26], SSD [27] and R-FCN [28]) and
a face mo del traine d with two dierent loss functions (i.e.,
cosine softmax classier loss [29] and angle softmax classi-
er loss [30]). Face feature s of the upp er facial regions (i.e.
eyes, eyebrows and forehead) are used to track masked fac es
in [31]. A multi-face tracking metho d based on SORT [15]
(i.e., motion based asso ciation) with a similarity matching
blo ck (comparing the stored with the currently computed
faces features (i.e., ArcFace [22])) as a fallback for all un-
matched detections showing a frontal face is prop osed in [32].
The metho d is named ReSORT, b ecause IDs can b e recov-
ered based on the similarity matching blo ck. Face features
as fallback if motion based asso ciation fails is also prop osed
in [33]. In [34], a single ob ject tracking metho ds to pre-
dict the p ositions of active tracks (motion cues) is used. The
p osition-based matches are then corrected using face features.
A multi-face tracking metho d based on face features only is
presented in [35]. Cumulative learning of face features is p er-
formed bas ed on a memory mo dule where selectively redun-
dant features are removed. The prop osed metho d is referred
as IdOL (Identity Online Learning). A Multi-Camera Face
Detection and Recognition (MCFDR) tracking is prop osed in
[36]. MCFDR combines YOLO [37] face detection with Deep-
SORT [16], where SphereFace [38] face features are used for
asso ciation.
The face features used for tracking are usually based on
pre-trained face recognition mo dels (e.g., [22, 39, 40, 41])
that are typically trained on more or less frontal, uno ccluded
faces. In a multi-face tracking scenario, however, lateral
faces (out-of-plane rotations) and partially o ccluded faces o c-
cur. To address this issue, a multi-mo dality tracker, Online
Multi-face Tracking with Multi-mo dality Cascaded Match-
ing (OMTMCM), is prop osed in [42]. The prop osed metho d
utilizes b o dy and face features and inc ludes two stages: (i)
detection alignment, and (ii) detection asso ciation. The de-
tected faces and b o dies are aligned in the rst stage (i.e.,
detection alignment). Detection asso ciation is p erformed in
a cascade, whereby matching based on face features is p er-
formed only with those detections that could not b e matched
with b o dy features. The face features are extracted by
a SeNet [43] pre-trained on the VGGFace2 database [44]
and the b o dy features by a ResNe t [45] pre-trained on the
MovieNet database [46]. However, a corresp onding b o dy is
not always visible for every detected face, esp ecially in the
assumed scenario in which p eople are moving towards a gate
in a queue.
In this work, we prop ose FaceSORT, a ne w TbD based
multi-face tracker inspired by StrongSORT. Instead of uti-
lizing features extracted from two mo dalities (e.g., face and
b o dy)[42], two dierent typ es of features are extracted from
the same m o dality (i.e., detected fac e). By using two dier-
ent features from the same image patch, the required pres-
ence of b o dy and face, which hardly exists in the cons id-
ered scenario, is eliminated. The two features (i.e., fac e fea-
tures and app earance fe atures ) are combined into a single
cost value, which forms the basis for asso ciation. To the b est
of our knowledge, FaceSORT is the rst multi-face track-
ing metho d that combines two features extracted from the
same image patch. All other describ e d multi-face trackers
[24, 25, 31, 32, 33, 34, 35] rely solely on face features. The
combination of the two features helps to b ette r handle lateral
or partially o cc luded faces. FaceSORT is evaluated bas ed on
a new multi-face tracking dataset. This datas et reects the
assumed scenario and is released as part of this work. To
the b est of our knowledge, the published dataset is the rst
multi-face tracking dataset in which p eople move in an (un-
organized) queue towards a camera (gate). There is only one
similar dataset, the ChokePoint dataset [47], in which sev-
eral p eople move through a p ortal. In the released dataset,
howe ver, a queue is simulated in which p eople are talking,
eating, pushing, shoving, etc.
In summary, the main contributions of this work are :
â€¹
A novel multi-face tracking metho d (FaceSORT) is pro-
p osed that combines two dierent features (i.e ., biomet-
ric (face) feature s and app earance features) extracted
from the same image patch.
â€¹
A com prehensive exp erimental evaluation is p erformed
comparing dierent face feature descriptors, analyzing
parameter selection and applying a dierent similarity
metric (i.e., Euclidean distance).
â€¹
An ablation s tudy is p erformed.
â€¹
A dedicated multi-face tracking dataset is rele ased. T his
dataset constitutes of seven annotated sequence s (12730
frames) that reect the assumed scenario (p eople moving
in queue towards a gate).
â€¹
Annotations are provide d for 6 sequences from the
ChokePoint dataset.
The remainder of this pap er is organized as follows: the
prop osed FaceSORT is describ ed in sec tion 2. Section 3 pro-
vides an overview of the prop osed dataset. The exp erimental
evaluation is describ ed in section 4. An ablation study is
conducted in section 5 and the key insights are summarized
in the last section 6.
2 FaceSORT
FaceSORT is a multi-face tracker based on the basic variant
of StrongSORT [17]. Since common face recognition mo d-
els are usually trained on constrained face image s, successful
2
asso ciation (o f detected faces and active traces) of lateral
or partially o ccluded faces based on extracted face features
can b e dicult. To mitigate this problem, FaceSORT com-
bines two dierent typ es of features extracted from the same
image patch (i.e., detected faces). T he se features are: (i)
face features (representing biometric information), and (ii)
app earance features (representing visual app earance). The
combination of these features is p erformed on the cost ma-
trix level.
In common TbD based metho ds, the ass o ciation relies on
a cost matrix
C
n

m
, where
n
is the numb er of active tra-
jectories (tracks) a nd
m
is the numb er of detected ob jects.
An element
C
i;j
represents the cost of matching the
i
-th ac-
tive track to the
j
-th detected ob ject. The main ob jective
of asso ciation is to nd bipartite matches where the global
cost is minimum (i.e., usually solved by the Hungarian algo-
rithm [14]). In principle,
C
i;j
is based on a distance
d
from
a stored/predicted cue of the
i
-th active track to a calcu-
lated/observed cue of the
j
-th detected ob je ct and is calc u-
lated in FaceSORT as follows,
C
i;j
=
 C
app=bio
i;j
+ (1


)
C
pos
i;j
;
with
C
app=bio
i;j
=
C
bio
i;j
+ (1


)
C
app
i;j
;
and
C
bio
i;j
=
d
bio
(
bio
i
;
	
bio
j
)
;
C
app
i;j
=
d
app
(
app
i
;
	
app
j
)
;
C
pos
i;j
=
d
pos
(
pos
i
;
	
pos
i
)
:
(1)
The parameter

2
R
[0
;
1]
weights the contribution of the
biometric (face) features (
C
bio
i;j
) and the app earance features
(
C
app
i;j
). Thus,
C
app=bio
i;j
represents the similarity in terms of
biometric cues and visual app earance from a stored face (ac-
tive track) and a detected face. A spatial distance b etween
a predicted and an observed p os ition is represented by
C
pos
i;j
and we ighted by the parameter

2
R
[0
;
1]
.
The set of de tected faces at frame
t
is denoted by 	, and
	
bio
j
, 	
app
j
and 	
pos
j
represent the extracted biometric fea-
tures, the extracted app earance features and the observed
p osition (b ounding b ox), resp ectively.  represents the ac-
tive track memory, storing biometric features 
bio
, app ear-
ance features 
app
and p ositions 
pos
for each active track.
A track b ecomes inactive (is deleted from ) if it could not
b e matched with a detected fac e in
N
max
consecutive frames.
When a ne w track is added to , it is in a tentative state and
is only conrme d if the track could b e matched in the next
N
init
frames. Otherwise, the tentative track is deleted from
. Similar to StrongSORT, an Exp onential Moving Average
(EMA) [48] approach is used to up date the stored biometric
and app earance features from the
i
-th matched track, i.e.,

t
i
=


t

1
i
+ (1


)	
t
i
;
(2)
where 	
t
i
represents the extracted features from the current
frame
t
of the face asso ciated with track
i
and the parameter
Figure 1: Overview FaceSORT asso ciation.

is a momentum term. As stated in [17], this up dating strat-
egy leverages the information of inte r-fram e feature changes
and can mitigate detection noises.  is maintained separately
for biometric and app earanc e features (i.e., 
bio
and 
app
).
The prediction of the p osition of the
i
-th active track (
pos
i
)
in the next frame is based on the NSA Kalman lter [49]. The
distance measures used in equation (1) are the cosine simi-
larity (
d
bio
and
d
app
) and the Mahalanobis distance (
d
pos
).
To avoid unce rtain matches,
C
app=bio
i;j
is gated by a maximum
spatial distance

pos
, i.e.,
C
app=bio
i;j
=
(
inf
d
pos
(
bio
i
;
	
bio
j
)
> 
pos
C
app=bio
i;j
otherwise
;
(3)
and a general threshold

is applied, i.e.,
C
i;j
=
(
inf
C
i;j
> 
C
i;j
otherwise
:
(4)
Based on the cost matrix
C
, the bipartite matching problem
is formulate d as follows,
ar g max

i;j
2f
0
;
1
g
P
j
(
t
)
j
i
=1
P
j
	(
t
)
j
j
=1

i;j
(
C
max

C
i;j
)
s.t.
(
P
i

i;j

1
;
8
j
= 1
; : : : ;
j
	(
t
)
j
P
j

i;j

1
;
8
i
= 1
; : : : ;
j
(
t
)
j
;
(5)
where
C
max
denotes the larges t element in
C
(less than inf ).
The problem is solved by the Hungarian algorithm [14].
In Dee pSORT, a matching cascade is applied in which the
matching is p erformed in sequence, starting with the tracks
that were correctly matched in the previous frame and ending
with the tracks that have not b een matched for the longest
time. The matching cascade reduces the se arch space and
could b e b enecial when long o cc lus ions are allowed. An
overview of the core asso c iation pro ce dure of FaceSORT is
depicted in Figure 1. For all unmatched detections (after the
matching cascade), an IoU-based asso ciation (as prop osed in
SORT [15]) is p erforme d as a fallback routine. The ge ne ral
steps that are p erformed in FaceSORT for each frame are
summarized in Algorithm 1.
2.1 Computational Compl exity
As shown in Algorithm 1, FaceSORT essentially comprises
the matching cascade, the IoU fallback matching, the fea-
3
Algorithm 1
FaceSORT at frame
t
Input:
 and 	
1:
predict p osition 
pos
(Kalman Filter)
2:
	
0
= 	; 
+

 (conrmed tracks)
3:
for
i
= 1
; : : : ; max
(
+
ag e
)
do
4:
if
j
	
0
j
>
0
then
5:
calculate
C
by Eqn. (1) using

+
ag e
=
i
and 	
0
;
6:
nd matche s Eqn. (5)
7:
up date 	
0
(unmatched de tections)
8:
else
9:
break
10:
end if
11:
end for
12:
IoU fallback matching if
j
	
0
j
>
0
13:
up date/add 
bio
and 
app
for all detections (Eqn. (2))
ture up dating and the p osition prediction. In general, the
complexity of nding bipartite matches using the Hungar-
ian algorithm is
O
(
j

j
2
 j
	
j
). T his is also the worst case
with cascade matching. In the considered scenario, it is very
likely that the detected face 	 in frame
t
were also present
in frame
t

1. Thus, it is reasonable that the complexity can
b e reduced by the m atching cascade (i.e., to
O
((
j

j
=
x
)
2

	)).
For all unmatched detections after the matching cascade, the
complexity of the IoU fallback matching is in worst c ase again
O
(
j

j
2
 j
	
j
). The complexity for up dating the memory for
biometric and app earance features (
bio
and 
app
) is
O
(
j
	
j
)
in each case. Finally, predicting the p ositions in frame
t
+ 1
using the Kalman lter has basically a complexity of
O
(	).
Thus, the overall complexity of FaceSORT is,
O
(
j

j
2
 j
	
j
+
j

j
2
 j
	
j
+ 2
j
	
j
+
j

j
)
;
(6)
which c orre sp onds to the complexity class
O
(
j

j
2
 j
	
j
).
3 PLUS Faces in a Queue Dataset
The Paris Lo dron University Salzburg Faces in a Queue
(PLUSFiaQ) dataset is a new multi-face tracking dataset con-
sisting of a total of seven dierent sequences. This dataset
is made publicly available
1
. An overview of the individual
sequences is given in Table 1. In total, the seve n sequences
comprise 12730 fully annotated frames. This corresp onds to
ab out 8 minutes and 30 seconds of video mate rial (25 frames
p er second). The recorded sequences reect the assumed sce-
nario (describ ed in section 1), in which several p eople move
towards a gate in a queue. Informe d consent was obtained
from all partic ipants for all released sequences.
3.1 Scenario Detail s
The assumed scenario (i.e., queue in front of a gate) is simu-
lated with 12 die re nt p eople. These p ersons move towards
1
https://www.wavelab.at/sources/Joechl24b/
Table 1: Sequence (SEQ) details, where F denotes the frames,
P the individual p e rs on and O
d
the o cclusion duration.
SEQ-ID
# F
# P

(O
d
)
Ë™
(O
d
)
min(O
d
)
max(O
d
)
01
1774
3
52.00
32.53
29
75
02
3051
12
44.52
55.30
1
500
03
701
12
42.04
50.38
1
250
04
576
10
38.26
38.57
2
206
05
3126
11
39.35
41.88
3
271
06
1301
12
43.68
39.17
4
197
07
2201
10
43.50
45.97
1
257
Figure 2: A schematic depiction of the reco rded scenario.
The area s haded in blue represe nts the visible area of the
camera.
a camera equipp ed gate. On their way to the gate, they
form a queue. However, this queue is not very well orga-
nized, i.e., p eople are chatting, eating, moving quickly, walk-
ing backwards, pushing or shoving. As so on as a p erson has
entered the gate , they leave the visible area of the camera and
thus the sc ene. To simulate multiple runs, each p erson go es
more than once through the gate. For this reason, the p eo-
ple move in a counterclo ckwise circle (as depicted in Figure
2). Thus, when a p erson re-enters the scene, the p erson rst
move s away from the gate (camera). This is also illustrated
in Figure 3, where the tra jectory of the top-left Bounding-
Box co ordinates from a sp ecic p erson is shown (i.e., trID
1702, marked by a red circle). At p osition (a) the p erson re-
enters the scene, then moves away from the gate (b) until he
is roughly farthest away at (c) and then moves towards the
gate again at (d) and (e). Finally, the p erson again leaves
the scene (go es through the gate) at p osition (f ). This lo op
is rep eated several times.
The only exception is sequence 1, in which only three p eo-
ple are involved, and they move clo ckwise , i.e., when they
re-enter the scene, they move directly towards the gate. This
sequence is also the most simple sequence.
3.2 Annotation Details
Annotations provided comprise a Bounding-Box (BB) for
each face, a corresp onding tracking ide ntity (trID) and a tar-
get c lass ag. In addition, a visibility meas ure and a `next at
4
tra je ctory
(a)
(b)
(c)
(d)
(e)
(f )
Figure 3: Example of a tra jectory of the p erson with trID 1702.
gate' ag are included, but these have not b een fully adjusted
and corrected at this stage. Furthermore, the alignment of
the BBs is not dened, thus the contained areas of the face
can dier b etween the BBs.
The trID is a four-digit numb er (e.g. xxyy), where the rst
two digits (xx) represent the p ersonal identiers and the las t
two digits (yy) represent a consecutive tracking numb er (p er
sequence). A new tracking numb er is assigned when a p erson
has left the scene (go es through the gate) and re-enters the
scene again. For example, the p ers on in gure 3 (a), who is
directly in front of the gate, has the p ersonal identier 18.
The current tracking numb er is 1 (i.e., trID 1801). However,
after leaving and re-entering the scene, the tracking numb er is
incremented by one (i.e . trID 1802), as shown in Figure 3 (d).
For all non-target class ob jects (faces) a p ersonal identier of
99 is assigned. The individual non-target class instances are
assigned a dedicated tracking numb er, indep endent of how
often they have re-entered the scene.
For annotation, the VGG Image Annotator (VIA)
2
[50] was
used. The corresp onding exp orted annotation le (json le)
is provided for each s equence. Furthermore, a Ground-Truth
(GT) le according to the MOT20 format [51] is provided
additionally, i.e., a csv le where each line represents one
ob jec t instance and c ontains 9 values:
<
fram e ID
>
,
<
trID
>
,
<
BB left
>
,
<
BB top
>
,
<
BB width
>
,
<
BB
height
>
,
<
conf
>
,
<
class
>
,
<
visibility
>
In case of ground-truth (GT), the 7
th
value (detector con-
dence score) acts as ag whether the entry is to b e considered
(i.e., 1: target class, 0: ignore). Since only multiple ob ject
(face) tracking is considered, the class lab el is constantly set
to 1. Visibility represents the ratio how much of that ob ject
is visible.
As a starting p oint for the annotation, the BBs detected
by yolov5
3
and the trIDs predicted by StrongSORT [17] were
used. These BBs and trIDs were then rened manually.
2
http://www.rob ots.ox.ac.uk/ vgg/software/via
3
https://github.com/ultralytics/yolov5
motion blur
comp. artefacts
out-of-plane rot.
Figure 4: Examples of tracking challenges.
Figure 5: Distribution of o cclusion duration. Inter-track o c-
clusions represent the time it takes (in frames) for a p erson
to leave and re-e nter the sce ne .
3.3 Tracking Challenges
The PLUSFiaQ dataset comprises various dierent multi-face
tracking challenges. These challenges include: (i) fast m o-
tion, (ii) lo oking down or sidewards , (iii) cover the face (e .g.,
wear a mask), (iv) motion blur (see Figure 4), (v) deforma-
tions (i.e., eating or grimacing), (vi) compression artefacts
(see Figure 4) and (vi) out-of-plane rotations (see Figure 4).
However, the most dicult challenge are likely o cclusions.
In Figure 5, the distribution of o cclusion duration in frames
is illustrated. As can b e seen, around 80% of all o cclusions
are b etween 1 and 100 frames in length. Inter-track o cc lu-
5
sions are those o cc lus ions that o ccur when a p erson has left
the scene (go es through the gate, the tracking numb er is in-
cremented by one, e.g., 1801 to 1802), and the inter-track
o cclusion duration is the time from leaving to re-e ntering the
scene. On average, inter-track o cclusions are 257.131 frames
long, with maximum o cclusion of 1400 frames . The average
o cclusion duration (intra-track) p e r se quence as well as min-
imum and m aximum o cclusion duration and the standard
deviation can b e seen in Table 1. In principle, when referring
to o cclusions, intra-track o cclusions are meant. For example,
the p erson with trID 1702 (shown in Figure 3) is o cc luded
11 times on the way from entering to le aving the sce ne , with
a maximum o cclusion duration of 83 frames and a minimum
of 3 frames. The average numb er of o cclusions p er trID is
0.18, 5.91, 2.42, 3.07, 4.84, 4.89 and 4.03 for sequences 1 to
7, resp ectively.
4 Exp erimental Evaluation
FaceSORT is evaluated based on the prop osed PLUSFiaQ
(see section 3) and the ChokePoint[47] dataset. In the Choke-
Point dataset, p eople moving through a p ortal (chokep oint)
are recorded. T he dataset comprises sequences from 18 dier-
ent runs (i.e., dierent p ortal, session or walking direc tion),
with 3 dierent camera p ersp ectives available for each run
(i.e., 18

3 = 54 available sequences). Of the se 54 sequences,
only 6 sequences show multiple p ersons p er frame, which is
similar to the assumed sce nario in this pap er. These 6 se-
quences are therefore used for e valuation. However, no an-
notations are available for these 6 sequences and are created
accordingly. In the ChokePoint dataset, p eople are reected
in the op en glass do or as they walk through the p ortal. T he se
refelections are annotated as non-target classes. The anno-
tations created have the same format as describ ed in section
3 and are made available along with the PLUSFiaQ dataset.
4.1 Tracker Implementation
FaceSORT is implemente d based on the StrongSORT imple-
mentation
4
. The parameter

for the EMA feature up dat-
ing s trate gy is set to

= 0
:
9. The weighting parameter

(for weighting the contribution of biometric and app ear-
ance similarity compared to s patial dis tanc e) is set to 0.98.
This gives the biometric and app earance similarity the mos t
weight.

pos
is set to 9.4877 (the 0.95 quantile of the chi-
squared distribution with 4 degrees of freedom). Thes e pa-
rameters are set according to [17]. The ge ne ral cost threshold

is set to 0.2 and the minimum IoU (fallback routine) to 0.3.
For all exp eriments,
N
init
= 1 and the maximum age pa-
rameter is set to
N
max
= 100 frames (i.e., all tracks that
were not successfully m atched for more than 100 frames are
deleted). With this setting, most o cclusions are taken into
account while avoiding interference from inter-track o cclu-
4
https://github.com/dyhBUPT/StrongSORT
sions (as shown in Figure 5). As face detector yolov8
5
with
a condence score threshold of 0.4 is used.
4.2 Evaluated Features
To extract the app earance features, a generic ob ject clas-
sication mo del is used, i.e., a ResNet [45] trained on the
ImageNet dataset [52] (resenet18 imp orted from torchvision
mo dels). The app earance features are combined with several
dierent face (biometric) features. The python framework
DeepFace [53, 54] provides a wrapp er for multiple state-of-
the-art face descriptors, i.e., VGG -Face based on the VGG-
Very-Deep-16 CNN architecture [39] and evaluated on the
Faces in the Wild [55] and the YouTub e Faces [56] datase t,
Facenet [40] trained on the CA SIA- WebFace [57] and the VG-
GFace2 [44] database, Op enFace [58] trained on the CASIA-
WebFace [57] and FaceScrub [59] dataset (based on pap e r),
DeepFace [60] trained on a large collection of Faceb o ok
images (i.e., the So cial Face Classication (SFC) dataset),
DeepID [61] trained on the Faces in the Wild [55] dataset
and tested, ArcFace [22, 62], SFace [41] trained on CASIA-
WebFace [57] VGGFace2 [44] and MS1MV2 and Dlib [63]
trained on the VGG-Face [39] and FaceScrub [59] datas et
(plus a large numb er of images the author scrap ed from the
internet). All the se dierent face descriptors provided by
DeepFace are evaluated. DeepFace also provides a 512 di-
mensional face descriptor for Facenet (i.e., Facenet512) and
ArcFace (ArcFaceI I) is also evalua te d based on a dierent im-
plementation
6
. In addition, the face descriptor used in [42],
SeNet [43], is evaluate d.
4.3 Evaluation Metrics
The Multiple Ob ject Tracking Accuracy (MOTA)[64] is a
commonly used metric for evaluating multiple ob je ct tracking
metho ds. With MOTA, tracking errors are accumulate d on
a frame -by-frame basis, i.e., wrongly detected ob jects (Fals e
Positive (FP)), missed ob jects (False Negative (FN)) and
identity switches (IDSWs) are counted for each frame (
t
),
summed up and normalized by the total amount of ob jects
in the ground- truth (GT),
M O T A
= 1

P
t
(
F P
t
+
F N
t
+
I D S W
t
)
j
GT
j
:
(7)
An IDSW o c curs if the ID assigned in the previous frame
(
t

1) diers from the ID assigned in the c urrent frame (
t
). As
MOTA only considers the previous frame, the entire tracking
p erformance of an ob ject during its lifetime is not taken into
account. For example, if an ob ject is tracked correctly for
90% of its lifetime but yields the same numb er of IDSW as
an ob ject that is only tracked correc tly for 60%, the same
MOTA score is achieved. Furthermore, the MOTA score ca n
b e dominated by the dete ctor p erformance (i.e., TP and FP).
5
https://github.com/ultralytics/ultralytics
6
https://github.com/mobilesec/arcface-tensorowlite
6
Since the face detection is xed for all exp eriments, only the
normalized IDSW is calculated, i.e.,
I D S W
=
P
t
I D S W
t
j
GT
j
:
(8)
A metric that fo cuses on the entire sequence (how long an
ob jec t is correctly tracked) is the IDF1 [65], i.e.,
I D F
1 =
2
I D T P
2
I D T P
+
I D F P
+
I D F N
;
(9)
where I DTP are the correct tracked ob jects, IDFP are
tracked ob jects that do not match any ground-truth ob-
ject and IDFN are ob jects in the ground-truth that are not
tracked. The global assignment of tracke r hyp othesis and
ground-truth ob jects is p erformed using the suggested truth-
to-result match pro cedure, i.e. the combination that yields
the highest IDF1 score is selected.
The Higher Order Tracking Accuracy (HOTA)[66],
H O T A

=
p
D etA


AssA

;
(10)
is an evaluation metric for assessing MOT trackers, where
D etA

and
AssA

denote the dete ction and asso ciation ac-
curacy at lo calization thre shold

. The lo calization thresh-
old represents the minimum IoU that a detected BB and a
ground-truth BB must reach in order to consider the corre-
sp onding ground-truth ob ject as detected (i.e . as TP detec-
tion). Thus, HOTA combines all three dierent asp ects for
MOT tracker evaluation (i.e., lo calization, detection and as-
so ciation p erformance) into a single s core. To evaluate the
dierent comp onents individually, HOTA can b e decomp osed
into submetrics. In the considered exp erimental evaluation,
only the asso ciation p erformance is relevant (the detected
BBs and lo cations are xed for all exp eriments).
To measure the asso ciation p erformance, the authors
in [66] prop ose the concept of True Pos itive Asso ciations
(TPAs), False Negative Asso ciations (FNAs) and False Pos-
itive Asso ciations (FPAs). TPAs are the set of all correctly
tracked TPs (correctly detected ob jects), while FNAs are the
set of TPs that are assigned dierent trIDs for the same
ground truth ID (gtID), and FNs (not de tected ob jects).
In other words, FNAs represent the amount of intra-p erson
trID switches. On the contrary, FPAs are the set of TPs
with the same trID but dierent gtIDs (i.e., inter-p erson trID
switches), and FPs (wrongly detected ob jects). Based on the
TPAs, FNAs and FPAs the asso ciation recall (Ass Re), i.e.,
AssRe

=
1
j
TP
j
X
c
2f
TP
g
j
TPA(
c
)
j
j
TPA(
c
)
j
+
j
FNA(
c
)
j
;
(11)
and asso ciation precision (AssPr), i.e.,
AssPr

=
1
j
TP
j
X
c
2f
TP
g
j
TPA(
c
)
j
j
TPA(
c
)
j
+
j
FPA(
c
)
j
;
(12)
can b e computed. A combination (Jaccard index) of AssRe
and AssPr is the asso ciation a ccuracy (AssA), i.e.,
AssA

=
AssRe


AssPr

AssRe

+ AssPr


AssRe


AssPr

:
(13)
In HOTA, AssA is the m ain measure for asso ciation p erfor-
mance. For all rep orted results,

= 0
:
2.
4.4 Exp erimental Res ul ts
In FaceSORT, the selection of the weighting parameter

(combination of biometric and app earance features) and the
asso ciation threshold

is crucial. Thus, the parameter selec-
tion is evaluated in detail along with dierent fac e feature s
and a dierent similarity metric (Euclidean distance instead
of cosine similarity).
4.4.1 Selection of a xed

Table 2: PLUSFiaQ: maximum AssA a nd IDF1 as well as
minimum IDSW, with the c orres p onding

selection.

AssA
"

IDF1
"

IDSW
#
ArcFace
0.1
0.6461
0.1
0.7342
0.0
0.0088
DeepFace
0.1
0.6494
0.1
0.7411
0.0
0.0088
DeepID
0.0
0.6303
0.0
0.7277
0.2
0.0083
Dlib
0.0
0.6303
0.0
0.7277
0.4
0.0082
Facenet
0.1
0.6355
0.1
0.7308
0.0
0.0088
Facenet512
0.1
0.6550
0.1
0.7397
0.0
0.0088
Op enFace
0.1
0.6547
0.1
0.7466
0.0
0.0088
SFace
0.1
0.6527
0.1
0.7338
0.0
0.0088
SeNet50
0.2
0.6425
0.2
0.7396
0.2
0.0084
VGG-Face
0.1
0.6515
0.1
0.7405
0.0
0.0088
ArcFaceI I
0.1
0.6451
0.1
0.7333
0.0
0.0088
Table 3: ChokePoint: maximum AssA and IDF1 as well as
minimum IDSW, with the c orres p onding

selection.

AssA
"

IDF1
"

IDSW
#
ArcFace
0.2
0.8364
0.1
0.8754
0.3
0.0068
DeepFace
0.6
0.8359
0.3
0.8755
0.0
0.0076
DeepID
0.6
0.8306
0.2
0.8756
0.0
0.0076
Dlib
0.5
0.8353
0.1
0.8752
1.0
0.0052
Facenet
0.3
0.8306
0.0
0.8724
0.0
0.0076
Facenet512
0.1
0.8355
0.1
0.8773
0.1
0.0075
Op enFace
0.2
0.8298
0.2
0.8728
0.0
0.0076
SFace
0.1
0.8404
0.1
0.8783
0.0
0.0076
SeNet50
0.4
0.8325
0.4
0.8740
0.5
0.0069
VGG-Face
0.3
0.8497
0.2
0.8819
0.0
0.0076
ArcFaceI I
0.2
0.8396
0.1
0.8777
0.0
0.0076
Figure 6 shows the average (across all sequences) AssA and
IDF1 scores as well as the normalized amount of IDSW for
all evaluated face descriptors and dierent

selections. In
general, the higher the AssA or IDF1 score or the lower the
numb er of IDSWs, the b e tter the tracking p erformance. The
scores achieved when setting

to 1.0 c an b e considered as
the baseline. In this setting, only face (biometric) features are
taken into account (essentially corresp onds to StrongSORT
with matching cascade). When lo oking at AssA and IDF1,
7
Dataset: PLUSFiaQ
Dataset: Cho kePoint
Figure 6: The average (across all sequences) AssA and IDF1 scores as well as the normalized amount of IDSW for all
evaluated face descriptors and

selections. Recall: for AssA and IDF1, the higher the b etter, and for IDSW, the lower the
b etter.
a general increase in scores can b e observed with increasing
inuence of the app earance fe ature (i.e.

!
0). The numb er
of IDSWs, on the other hand, decreases. Thus , the tracking
p erformance incre ases when

decreases (i.e., with increasing
inuence of the app earance feature).
The b est values achieved (i.e. max. AssA and IDF1 as well
as min. IDSW) are shown together with the corresp onding

values in tables 2 and 3. In general, signicantly higher
results are achie ved with the ChokePoint dataset (i.e., the
PLUSFiaQ dataset is more challenging). With the PLUS-
FiaQ dataset, the b est results (AssA and IDF1) are achieved
at

= 0
:
1 (except for Dlib, DeepID and SeNet50). In
contrast, the b est results are achieved with the Chokep oint
dataset having

values up to 0.6. One reason for this could
b e that with the ChokePoint dataset, p eople lo ok towards
the camera when they approach the p ortal, which facilitates
biometric features. The numb er of IDSWs is usually lowest
at

= 0
:
0 (i.e., when only app e arance features are use d).
However, it can b e s tated that the combination of biometric
and app earance features improves tracking p erformance.
In multi-face tracking, inter-p erson trID switches (i.e.,
when the trID is sw itched to an existing trID of a dierent
p erson) are likely to b e more critical than intra-p erson trID
switches (i.e., when a new trID is assigned). As already de-
scrib ed, the AssPr s core reects the FPAs (i.e., inter-p e rs on
trID switches) and is illustrated in Figure 7. If only face
(biometric) features are used for asso ciation (i.e.

= 1
:
0),
inter-p erson identity switches are exp ected to b e limited, as
the extracted face (biometric) features should b e distinct for
dierent p ersons. For b oth analyzed datasets, this can ba-
sically b e obse rved for 4/11 evaluated face descriptors (i.e.,
SFace, ArcFaceI I, VGG-Face and Facenet512), with SFace
showing the mos t consistent and b est results. However, the
AssPr decreas es as

decreases. This is reaso nable as the
asso ciation in reducing

is more and m ore based on generic
app earance features and the app earance of two dierent faces
may b e similar, while the face (biometric) features are dis-
tinct. Considering that the b est AssA for these 4 face de-
scriptors is reached at lower

values (i.e., 0.1-0.3 for the
ChokePoint and 0.1 for the PLUSFiaQ dataset), the higher
AssA is accompanied by a decrease in AssPr (i.e., an increase
in inte r- p erson trID switches (FPAs)). In contrast, the AssPr
8
Figure 7: Average AssPr (across all evaluated sequences).
Figure 8: Quality score dis tribution.
for Dlib and DeepID is much lower for

= 1
:
0 than compared
to the generic feature baseline (Figure 7 dotted black line).
This would indicate weak face de scripto rs for the considered
evaluated sequences.
4.4.2 Selection of a dynamic

based on Face Quality
By setting the paramete r

to a xed value (e.g.

= 0
:
1),
the weighting b etween biometric and optical features is con-
stant for each image and each detected face, regardless of the
biometric information present in the extracted face patch.
For this reason, a more natural approach would b e for

to
reect the available biometric information (i.e.,

= 1 when
full biometric information is available and

= 0 when no bio-
metric information is available). The biometric information
present in a face sample can b e quantied by a face quality
score [67, 68, 69 ]. For this reason, another option would b e
to set

dynamically according on a face quality score. This
turns

into an
m

n
matrix with constant columns corre-
sp onding to the face quality scores of the
n
detected faces.
The multiplications
C
bio
and (1


)
C
app
are then element
wise multiplications.
Three dierent face quality mo dels are exploited, i.e., CR-
FIQA [67], MagFace [68] and FaceQgen [69]. In case of the
CR-FIQA two dierent backb one mo del sizes are evaluated,
i.e., CR-FIQA-S and CR-FIQA-L. To obtain a face quality
score the detected face patch is fed into the resp ective mo de l.
The distribution of quality scores achieved for all detecte d
faces (across all sequences) is illustrated in Figure 8. Qual-
ity values that are not restricted to
R
[0
;
1]
are normalized by
dividing by the maximum observed value.
In Figure 9 , the res ults are compared b etween a xed (b est
results table 2 and 3) and a dynamically set (based on face
quality scores)

. It can b e seen that tracking p e rformance
is generally lower when

is dynamically set based on face
quality scores. Only with CR -FIQA is the p erformance for
some face des criptors as high as when

is set to a xed
value. In general, the p erformance based on CR-FIQA is
consistently b etter than the other two face quality mo dels
evaluated. The b etter p erformance of CR-FIQA could b e ex-
p ected when lo oking at the distribution of face quality scores
(Figure 8), as FaceQgen and MagFace only cover a limited
range, while CR-FIQA covers almos t the entire range from 0
to 1, w hich app ears more natural. Thus, the reasonable ap-
proach of dynamically se tting

based on available biometric
information (face quality scores) is not working. A reason for
this c ould b e that the considered face quality scores do not
reect the actuall biometric information present.
4.4.3 Grid Search
To assess whether a higher AssA can b e achieved with a dy-
namic

(which is dierent for each detected fac e) than with
a xed

, a grid search is p erformed. In a grid search, all
p ossible combinations of

are evaluated. For example, if 5
faces are detected p er frame and 11 dierent

values are al-
lowe d (i.e.

2
[0
:
0
;
0
:
1
;
0
:
2
;
0
:
3
;
0
:
4
;
0
:
5
;
0
:
6
;
0
:
7
;
0
:
8
;
0
:
9
;
1
:
0]),
this would result in 11
5
dierent combinations for the frame
under consideration. Since the asso ciation in the current
frame dep ends on the past frames, the dierent combina-
tions must b e cons idered across all frames, i.e.
Q
N
i
=1
11
m
i
combinations, where
N
is the numb er of frame s and
m
i
are
the detected faces for the
i
-th frame. This is simply not
computable. Thus, only a lo cal grid search (b etween two
consecutive frames) is p erformed. More precisely, the AssA
9
Figure 9: Achieved AssA, IDF1 and IDSW with

dynamically set based on normalized (by the maximum value) face quality
values compared to the b est achieved Ass A, IDF1 and IDSW based on a xed

value. Recall: for AssA and I DF1, the
higher the b etter, and for IDSW, the lower the b etter.
(a) sequence ID 04
(b) sequence ID 03
Figure 10: Highest AssA achieved p er frame in the grid
search.
is calculated after each

combination for the current frame.
The combination that achieves the highes t AssA is selected.
If several combinations achieve the highe st AssA, the combi-
nation that achieves the highest AssA rst is selected.
Since the evaluation of 11 dierent

values is still very
time-consuming, a smaller numb er is considered. In addition,
the grid search is only p erformed with the b est face descriptor
(in terms of AssPr), SFace, and only for the two sequences
(a) seq uence ID 04
(b) seq uence ID 03
Figure 11: Face patches that were assigned a

of 1.0 during
the grid search.
with the fewest frames (sequence ID 3 and 4). In Figure 10
(a), the highest AssA achieved for each frame of sequence
4 is illustrated. It c an b e seen that the AssA at frame 576
(complete s equence) is ab out 0.09 higher than the baseline
(xed

) and exactly the same results are obtained regardless
10
of whether

is selected from [1.0,0.9,0.7,0.5,0.3,0.1,0.0 ] or
[1.0,0.1,0.0].
Taking into account p ossible

values of
[1.0,0.9,0.7,0.5,0.3,0.1,0.0], a

of 1.0 is selected for
2199/2239 detec ted faces in sequence 04. The

values for
the remaining 40 detecte d faces are distributed as follows :
5, 22, 7, 4, 2 for the

values 0.0, 0.1, 0.3, 0.5 and 0.7,
resp ectively. The vast ma jority of

values are se t to 1.0
b ecause the grid search conducted always selects the rst

combination that achieves the highest AssA value, and the
rst combination tried is that all

values equal 1.0. In the
version where

can b e chos en from only three values (i.e.
[0.1,0.0,1.0]), the rs t combination tried is that all

values
are equal to 0.1, and the last combination is that all

values
are equal to 1.0 (i.e. a

of 1.0 is only chosen if 0.1 and 0.0
do not achieve an equally high Ass A). In this case, a

of 0.1
is selected for 2206/2239 detected faces. Compared to the
baseline (i.e. xed

of 0.1 for all recognized faces), a change
of

for only 33 recognized faces yields the p erform anc e
gain. In particular, for 24 detected faces, a

value of 1.0
is selected. A

value of 1.0 means that only face features
are used. Therefore, it is exp ected that the corresp onding
face patches c ontain adequate biometric information. T he
24 face patches where

is set to 1.0 are illustrated in Figure
11 (a). As can b e see n, the biometric information is very
limited.
The results for sequence 3 are similar. In Figure 10 (b),
the highe st AssA achieved for each frame is illus trate d. If
the p ossible

values are [0
:
1
;
0
:
0
;
1
:
0
;
0
:
5], a value of 0.1 is
selected for 2431/2466 detected faces. A

value of 0.0, 0.5
and 1.0 is selected for 6, 1 and 28 recognized faces, resp ec-
tively. Again, it would b e exp ected that with a sele cted

value of 1.0, there should b e reasonable biometric informa-
tion in the face patches. However, as can b e seen in Figure 11
(b) the biometric information in all 28 face patches is limited.
These observations indicate tha t linking

to the available
biometric information do es not lead to b etter p e rformance.
4.4.4 Matching Score Distribution
A multi-face tracker can b e compared with a biometric sys-
tem (e.g., a set of que ry images (detected fac es) are matched
against a database of enrolled identities (active tracks)).
Matching is based on a similarity s core, and in a p erfect
system, the genuine scores do not overlap w ith the imp os-
tor scores. In the conte xt of FaceSORT, the genuine scores
represent the cosine similarity that result when a detected
face is compared with the corresp onding stored faces (active
tracks). The imp oster sc ores, on the other hand, represent
the cosine similarities that are achieved when a detected face
is compared with all other stored faces (tracks). Table 4
shows the mean and the standard deviation of the genuine
and the imp os ter score distribution as well as their overlap
(IoU). Considering the IoU, the most distinct face descriptor
(lowes t IoU) is SFace. Other face descriptors with a low IoU
Table 4: Presents the overlap (IoU) b etween the genuine and
imp oster sc ore s as well as their mean (

) and standard de-
viation (
Ë™
). Feature mo dels: (1) ArcFace, (2) DeepFace, (3)
DeepID, (4) Dlib, (5) Facenet, (6) Facenet512, (7) Op enFace,
(8) SFac e, (9) SeNet50, (10) VGG-Face, (11) ArcFaceI I and
(12) App earance Features.
PLUSFiaQ ChokePoint
genuine imp oster genuine imp oster
IoU


IoU


(1)
0.26
0.14/0.14
0.52/0.31
0.20
0.15/0.14
0.57/0.29
(2)
0.14
0.10/0.07
0.25/0.08
0.12
0.10/0.08
0.26/0.08
(3)
0.20
0.02/0.03
0.09/0.06
0.17
0.03/0.04
0.10/0.06
(4)
0.07
0.02/0.01
0.07/0.02
0.08
0.02/0.01
0.08/0.03
(5)
0.18
0.12/0.12
0.51/0.25
0.26
0.13/0.13
0.46/0.27
(6)
0.07
0.16/0.14
0.76/0.20
0.09
0.19/0.15
0.73/0.22
(7)
0.18
0.10/0.08
0.31/0.14
0.18
0.11/0.09
0.35/0.17
(8)
0.04
0.27/0.17
0.87/0.13
0.04
0.29/0.17
0.90/0.12
(9)
0.18
0.02/0.03
0.09/0.06
0.19
0.05/0.07
0.25/0.16
(10)
0.07
0.15/0.11
0.60/0.15
0.08
0.20/0.12
0.67/0.17
(11)
0.08
0.21/0.15
0.77/0.18
0.05
0.23/0.14
0.82/0.16
(12)
0.16
0.03/0.03
0.28/0.19
0.18
0.03/0.04
0.26/0.18
Figure 12: AssRe over AssPr for die re nt

and

combi-
nations and SFace features. The

values increase from left
to right and the contour lines represent the AssA, where the
area shaded in red indicates a lower AssA than for the b est
achieve d AssA when only app earance features are used.
are Face ne t512, VGG-Face, ArcFaceI I and Dlib. These fac e
descriptors (exce pt Dlib) are als o the face descriptors achiev-
ing the highest Ass Pr (see Figure 7). However, based on the
IoU results, it is surprising that such low AssPr values are
obtained with the Dlib face descriptor. A reason for the low
AssPr values could b e that, with a mean value of 0.07, the
imp oster scores are well b elow the threshold

= 0
:
2 (applied
to reject uncertain (imp os ter) matches). Thus, a closer lo ok
is taken at the selection of the threshold value

.
4.4.5 Selection of the Threshold

For b oth datasets, the genuine and imp ostor scores obtained
on the basis of the Dlib features are close to zero (see Table
4). Thus, with a threshold

= 0
:
2 p ossible im p oster matches
(inter-p erson identity switches) are not rejected. This ex-
plains the low AssPr value when only Dlib features are use d
for the as so ciation (

= 1
:
0), although the genuine and im-
p oster distributions are relatively distinct (an IoU value of
0.07 and 0.08). If

is set to 0.0 25, an AssPr of 0.9831
11
Table 5: Best achieved AssA, IDF1 and IDSW for dierent selections of

and

combinations. Face descriptors: (1) ArcFace,
(2) DeepFace, (3) DeepID, (4) Dlib, (5) Facenet, (6) Facenet512, (7) Op enFace, (8) SFace, (9) SeNet50, (10) VGG-Face and
(11) ArcFaceI I.
PLUSFiaQ ChokePoint

/

AssA
"

/

IDF1
"

/

IDSW
#

/

AssA
"

/

IDF1
"

/

IDSW
#
(1)
0.2/0.1
0.6461
0.2/0.1
0.7342
0.35/0.1
0.0080
0.2/0.2
0.8364
0.2/0.1
0.8754
0.56/0.1
0.0054
(2)
0.2/0.1
0.6494
0.2/0.1
0.7411
0.3/0.1
0.0078
0.25/0.4
0.8446
0.25/0.4
0.8840
0.3/0.2
0.0053
(3)
0.175/0.1
0.6399
0.175/0.1
0.7349
0.2/0.2
0.0083
0.15/0.4
0.8376
0.15/0.4
0.8778
0.2/0.0
0.0076
(4)
0.175/0.0
0.6373
0.175/0.0
0.7287
0.2/0.4
0.0082
0.2/0.5
0.8353
0.2/0.1
0.8752
0.2/1.0
0.0052
(5)
0.175/0.0
0.6373
0.21/0.1
0.7337
0.35/0.1
0.0079
0.21/0.1
0.8314
0.21/0.1
0.8759
0.56/0.5
0.0051
(6)
0.3/0.3
0.6693
0.3/0.3
0.7520
0.4/0.2
0.0078
0.3/0.3
0.8493
0.4/0.4
0.8812
0.7/0.2
0.0049
(7)
0.25/0.3
0.6589
0.25/0.3
0.7505
0.3/0.2
0.0077
0.2/0.2
0.8298
0.2/0.2
0.8728
0.35/0.4
0.0052
(8)
0.4/0.4
0.6663
0.4/0.3
0.7559
0.6/0.5
0.0072
0.5/0.5
0.8879
0.5/0.5
0.9075
0.8/0.2
0.0046
(9)
0.2/0.2
0.6425
0.2/0.2
0.7396
0.25/0.3
0.0077
0.25/0.7
0.8457
0.25/0.3
0.8811
0.35/0.6
0.0050
(10)
0.2/0.1
0.6515
0.2/0.1
0.7405
0.3/0.1
0.0079
0.3/0.6
0.8581
0.3/0.6
0.8875
0.8/0.8
0.0048
(11)
0.3/0.2
0.6572
0.3/0.2
0.7433
0.4/0.2
0.0078
0.4/0.5
0.8611
0.4/0.5
0.8928
0.6/0.6
0.0047
?
0.23/0.16
0.6505
0.24/0.16
0.7413
0.33/0.22
0.0078
0.27/0.40
0.8470
0.28/0.33
0.8828
0.49/0.42
0.0053
and 0.9938 is achieved for the PLUSFiaQ and ChokePoint
datasets resp ectively. This shows that, in addition to

,

is an imp ortant parame ter as well. In Table 5, the b est
achieved AssA, IDF1 and IDSW values for dierent

and

combinations are shown. It can b e seen that the initial
choice of

= 0
:
2 was already pretty go o d for several face de-
scriptors. Overall, the b est results are achieved with SFace,
which also provides the most distinctive, genuine and im-
p oster score distributions (IoU of 0.04). Again, the

val-
ues with which the b est results are achieved are s ignicantly
higher for the ChokePoint dataset (probably les s o cclusions
and lateral faces b enet biome tric features). Although very
low

values (e.g. 0.0125, 0.025, 0.05, e tc.) were evaluated, no
combination of Dlib and app earance features achieved higher
tracking p erformance than with app earance feature only (i.e.

= 0
:
0).
When considering the threshold

the exp ectation is that
the AssPr is very high for low

values (reject most imp oster
matches) and it decreases when

increases. However, a low
value of

can lead to a lower AssRe (more intra-p erson trID
switches) since also genuine matches are re je cted. In Figure
12 the trade- o b etween Ass Re and AssPr compared to AssA
(contour lines) for the b est face descriptor SFace is shown for
dierent

and

combinations. The

values decrease from
left to right (i.e., 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.05
and 0.025) and the red lines show the AssRe and AssPr for
the b est AssA achieved w ith app earance features only (base-
line). The area shaded in red represents the area where the
AssA is lower than the baseline. A go o d trade-o  (relatively
constant high AssPr while reducing intra-p erson IDSWs (As-
sRe)) when increasing the threshold

can b e achieved for
higher

values (higher contribution of biometric features).
4.4.6 Selection of the Similarity Metric
Another factor that can inuence the tracking p erformance
is the selection of the similarity metric (used to compue
C
app=bio
i;j
). The b est results achieved when using Euclide an
distance instead of cosine similarity are shown in Table 6.
Again, the overall b est results are achieved with SFace fea-
tures, which are partly b etter than whe n using cosine similar-
ity. However, the average p erformance across all face descrip-
tors ass essed is slightly lower. Another noticeable die re nc e
is that the

and

values with which the b est results are
achieved are signicantly higher. The mean and standard
deviation of the genuine and imp oster score distributions in
terms of the Euclide an distance is shown in Table 7. It can
b e seen that they are clearly higher than in Table 4 (cos ine
similarity). The IoUs, however, are identical. The identical
IoUs could b e due to the fact that the features are normalized
b efore calculating the Euclidean distance. Thus, when using
the Euclidean distance, the scores are more or less scaled dif-
ferently. For the dierently scaled scores, b etter p erformance
can also b e achieved with Dlib features using a combination
of biometric and app earance features (i.e.

= 0
:
6).
4.4.7 Compare Tracker
In table 8 FaceSORT is compared with dierent s tate-of-
the-art multi-face trackers. A comparison with StrongSORT
including MC (i.e., when

is equal 1.0 or 0.0) is already
p erformed in `1) Selection of a xed

'. The FaceSORT re-
sults are obtained with SFace features,

= 0
:
1 and

= 0
:
2.
Implementations are available for the MC FDR
7
and UMA
8
.
However, no implementation is available for the OMTMCM
multi-face tracker. For this re ason, OMTMCM is carefully
implemented according to the description in [42]. To verify
the implementation, the results for the MusicVideo dataset
[70] are also presented in table 8. The MusicVideo da tas et
consists of 8 music videos (i.e., Apink, BrunoMars, Darling,
GirlsAloud, HelloBubble, PussycatDolls, Tara and Westlife)
from YouTub e, is publicly available
9
and was used in [42] to
evaluate the prop osed OMTMCM multi-face tracker. Since
music videos are unconstrained videos (with dierent scenes,
7
https://github.com/yjwong1999/Op enVINO-Face-Tracking-using-
YOLOv8-and-DeepSORT
8
https://github.com/yinjunb o/UMA-MOT
9
https://si tes.go ogle.com/site/shunz hang876/eccv16-face-tracking
12
Table 6: L2: Best achieved AssA, IDF1 and IDSW for dierent selec tions of

and

combinations. Face des criptors: (1)
ArcFace, (2) DeepFace, (3) DeepID, (4) Dlib, (5) Facenet, (6) Facenet512, (7) Op enFace, (8) SFace, (9) SeNet50, (10)
VGG-Face and (11) ArcFaceI I.
PLUSFiaQ ChokePoint

/

AssA
"

/

IDF1
"

/

IDSW
#

/

AssA
"

/

IDF1
"

/

IDSW
#
(1)
0.55/0.1
0.6287
0.55/0.1
0.7325
0.85/0.1
0.0080
0.55/0.3
0.8365
0.55/0.3
0.8801
1.15/0.3
0.0051
(2)
0.6/0.4
0.6398
0.6/0.4
0.7375
0.8/0.1
0.0080
0.7/0.6
0.8410
0.7/0.5
0.8839
0.9/0.9
0.0054
(3)
0.49/0.1
0.6215
0.49/0.1
0.7264
0.56/0.1
0.0086
0.6/0.4
0.8371
0.6/0.4
0.8803
0.8/0.8
0.0057
(4)
0.4/0.6
0.6444
0.4/0.6
0.7338
0.56/0.0
0.0091
0.4/0.6
0.8399
0.4/0.6
0.8793
0.4/0.8
0.0078
(5)
0.55/0.1
0.6395
0.55/0.1
0.7389
0.85/0.2
0.0080
0.55/0.3
0.8310
0.55/0.3
0.8723
1.15/0.4
0.0051
(6)
0.7/0.4
0.6762
0.7/0.4
0.7560
0.85/0.3
0.0079
0.85/0.5
0.8539
0.85/0.5
0.8882
1.15/0.4
0.0051
(7)
0.6/0.4
0.6398
0.7/0.6
0.7382
0.8/0.2
0.0080
0.6/0.3
0.8272
0.6/0.3
0.8731
0.9/0.6
0.0055
(8)
0.85/0.4
0.6672
0.85/0.4
0.7636
1.15/0.7
0.0074
1.15/0.9
0.8800
1.0/0.6
0.9010
1.3/0.3
0.0047
(9)
0.49/0.3
0.6376
0.49/0.3
0.7353
0.56/0.4
0.0081
0.6/0.4
0.8409
0.6/0.4
0.8781
0.8/0.6
0.0050
(10)
0.7/0.4
0.6565
0.7/0.4
0.7469
0.85/0.1
0.0079
0.7/0.6
0.8562
0.85/0.6
0.8872
1.15/0.9
0.0051
(11)
0.7/0.3
0.6613
0.7/0.3
0.7520
1.0/0.6
0.0077
1.0/0.8
0.8574
1.0/0.8
0.8925
1.0/0.7
0.0051
?
0.60/0.32
0.6466
0.61/0.34
0.7419
0.80/0.25
0.0081
0.70/0.52
0.8455
0.70/0.48
0.8833
0.97/0.61
0.0054
Table 7: L2: Prese nts the ove rlap (IoU) b e tween the genuine
and imp oste r scores as well as their mean (

) and s tandard
deviation (
Ë™
). Feature mo dels: (1) ArcFace, (2) DeepFac e,
(3) DeepID, (4) Dlib, (5 ) Facenet, (6) Facenet512, (7) Op en-
Face, (8) SFace, (9) SeNet50, (10) VGG-Face, (11) ArcFaceI I
and (12) App earance Features.
PLUSFiaQ ChokePoint
genuine imp oster ge nuine imp oster
IoU


IoU


(1)
0.26
0.45/0.26
0.95/0.37
0.20
0.48/0.27
1.01/0.35
(2)
0.14
0.41/0.15
0.70/0.11
0.12
0.43/0.15
0.72/0.11
(3)
0.20
0.18/0.12
0.41/0.14
0.17
0.20/0.13
0.43/0.13
(4)
0.07
0.19/0.06
0.37/0.06
0.08
0.21/0.06
0.38/0.07
(5)
0.18
0.43/0.22
0.97/0.28
0.26
0.44/0.25
0.90/0.32
(6)
0.07
0.52/0.23
1.22/0.17
0.09
0.57/0.23
1.19/0.20
(7)
0.18
0.43/0.16
0.77/0.17
0.18
0.45/0.16
0.81/0.20
(8)
0.04
0.71/0.22
1.31/0.10
0.04
0.74/0.20
1.34/0.09
(9)
0.18
0.19/0.10
0.41/0.13
0.19
0.28/0.15
0.67/0.23
(10)
0.07
0.52/0.19
1.09/0.14
0.08
0.60/0.18
1.14/0.17
(11)
0.08
0.61/0.22
1.23/0.15
0.05
0.65/0.19
1.27/0.13
(12)
0.16
0.20/0.11
0.70/0.29
0.18
0.23/0.13
0.67/0.28
moving camera, etc.), they do not corresp ond to the con-
sidered scenario in which p eople move towards a gate for
which FaceSORT is designed. For this dataset, OMTMCM
p erformed b est. However, in the considered sceneraio where
p eople are moving towards a gate (PLUSFiaQ and Choke-
Point), FaceSORT clearly outp erformed the other evaluated
state-of-the-art trackers.
5 Ablation Study
The main comp onents of FaceSORT are the combination of
biometric and app earance features, the matching cascade and
IoU fallback matching. Running FaceSORT with either bio-
metric (i.e.,

= 1
:
0) or app earance features (i.e.,

= 0
:
0)
has already b een evaluate d in `1) Selection of a xed

' of
subsection 4.4.
In the matching cascade (describ ed in section 2), the tracks
that were successfully matched in the previous fram e are c on-
sidered rst. This can reduce the search space  and can b e
particularly b enecial if long o cclusions are allowed (i.e., a
large active track memory ). For all exp eriments, o cclu-
sions up to 100 frames are allowed (i.e.,
N
max
= 100). In ta-
ble 9, execution times in frames p er second (fps) are rep orte d
for FaceSORT with and without the matching c ascade (MC)
and compared with the re- imple mentation of a state-of-the-
art tracker (i.e., OMTMCM [42]). All times were recorded on
a s tandard desktop PC (i.e., Intel Core i5 gen9) and without
taking face detection and feature extraction into account. It
can b e seen that the fps increases slightly when MC is applied.
This is also reected by the obs ervation that on average less
than half of all stored active tracks are required to success-
fully match all detected faces. However, applying the MC
carries the risk that not the b est matches are found. For ex-
ample, it could happ en that the b est match (lowest cost) for
a detected face is with a track tha t could not b e matched in
the last frames (e.g., a p erson returning from an o c clusion),
but the detected face is matched with a track that is tried
in the MC b efore. The achie ved tracking p erformance scores
for FaceSORT (with SFace features,

= 0
:
1 and

= 0
:
2)
without the m atching cas cade (
MC) are rep orted in table 8 .
Compared to results when using the matching cascade (table
8 rst row) a p erformance decrease can b e observed.
IoU fallback matching is applied to all unmatched detec-
tions after the matching cascade, including matching with
tentative tracks (only conrmed tracks are considered in the
matching c ascade). To achieve the rep orted tracking p er-
formance of FaceSORT (table 8 rst row), IoU matching is
p erformed for 3.91%, 4.9% and 8.42% of all detected faces
of the PLUSFiaQ, ChokePoint and MusicVideo dataset re-
sp ectively. When disabling IoU fallback matching (
IoU) the
p erformance decreases signicantly (see table 8). The p erfor-
mance achieved when Face SORT is applie d without matching
cascade and IoU fallback matching is rep orted in table 8 (last
row).
6 Conclusion
It this work, a new multi-face tracking metho d, FaceSORT, is
prop osed. To mitigate the problem of partially o ccluded and
13
Table 8: Compare FaceSORT with state-of-the-art multi-face tracker, where
MC and
IoU denote FaceSORT wihtout match-
ing cascade (MC) and/or IoU fallba ck matching.
PLUSFiaQ ChokePoi nt MusicVideo
AssA
"
HOTA
"
IDF1
"
IDSW
#
AssA
"
HOTA
"
IDF1
"
IDSW
#
AssA
"
HOTA
"
IDF1
"
IDSW
#
FaceSORT
0.6527
0.7486
0.7338
0.0104
0.8404
0.8672
0.8783
0.0081
0.0425
0.1821
0.0918
0.0343
OMTMCM[42]
0.2039
0.3765
0.3352
0.1671
0.2430
0.4483
0.4140
0.1335
0.1077
0.2842
0.2383
0.0944
MCFDR[36]
0.4770
0.5886
0.0740
0.0097
0.5514
0.5432
0.0144
0.0014
0.0274
0.1142
0.0063
0.0053
UMA[18]
0.1854
0.2531
0.2640
0.0437
0.5483
0.6932
0.6788
0.0112
0.0490
0.1933
0.0988
0.0268
FaceSORT
MC
0.5940
0.7147
0.7025
0.0169
0.8274
0.8603
0.8700
0.0107
0.0405
0.1776
0.0875
0.0400
FaceSORT
IoU
0.5764
0.7033
0.6760
0.0187
0.6933
0.7863
0.7671
0.0283
0.0363
0.1676
0.0830
0.0494
FaceSORT
MC
IoU
0.5589
0.6924
0.6666
0.0292
0.6697
0.7721
0.7521
0.0392
0.0359
0.1664
0.0812
0.0618
Table 9: Frames p er sec ond for FaceSORT (FS), FaceSORT
without the matching cascade (FS
MC) and OMTMCM [42].
PLUSFiaQ ChokePoint
FS
FS
MC
OMTMCM
FS
FS
MC
OMTMCM
90.43
87.25
71.14
143.03
134.99
134.01
lateral faces, two dierent features (i.e., fac e (biometric) and
app earance features) are combined. In the considered sce-
nario, w he n p eople move towards a gate/p ortal (PLUSFiaQ
and ChokePoint dataset) Face SORT clearly outp erformed
the evaluated state-of-the-art tracker. To get a deep er in-
sight into the prop osed metho d, a comprehensive exp erimen-
tal evaluation and an ablation study are conducted. It is
shown that the selection of the face desc riptor and the simi-
larity measure, as well as the resulting distribution of genuine
and imp oster scores are crucial. In general, selecting a low
similarity threshold

and a high parameter

lead to a high
AssPr.
For future work, an adaptive

could b e evaluated based on
the detec te d faces, i.e. in crowded scenes with probably more
partially o ccluded and lateral faces,

could automatically de -
crease. Furthermore, the general similarity threshold

could
b e split into two thresholds, one for biometric and one for
app earance features, to b ette r account for the dierent dis-
tributions of imp os ter and genuine scores. A di erent idea
would b e to p erform asso ciation based on biometric features
rst and apply app earance based asso ciation as fallback.
References
[1]
 W. Luo, J. Xing, A. Milan, X. Zhang, W. Liu, T.-K.
Kim, Multiple ob ject tracking: A literature review, Ar-
ticial Intelligence 293 (2021) 103448.
doi:10.1016/j.
artint.2020.103448
.
[2]
 T. Meinhardt, A. Kirillov, L. Leal-Taixe, C. Feichten-
hofer, Trackforme r: Multi-ob ject tracking with trans-
formers, in: Pro ce edings of the IEEE/CVF conference
on computer vision and pattern rec ognition, 2022, pp.
8844{8854.
[3]
 K. Deng, C. Zhang, Z. Chen, W. Hu, B. Li, F. Lu,
Jointing recurrent across-channel and spatial attention
for multi-ob ject tracking with blo ck-erasing data aug-
mentation, IEEE Transactions on Circuits and Systems
for Video Technology 33 (8) (2023) 4054{4069.
doi:
10.1109/TCSVT.2023.3238716
.
[4]
 Y. Jin, F. Gao, J. Yu, J. Wang, F. Shuang, Multi-ob ject
tracking: Decoupling features to solve the contradictory
dilemma of feature requirements, IEEE Transactions on
Circuits and Systems for Video Technology 33 (9) (2023)
5117{5132.
doi:10.1109/TCSVT.2023.3249162
.
[5]
 D. Chen, H. Shen, Y. Shen, Jdt-nas: Designing ef-
cient multi-ob ject tracking archite ctures for non-gpu
computers, IEEE Transactions on Circuits and Sys-
tems for Video Technology 33 (12) (2023) 7541{7553.
doi:10.1109/TCSVT.2023.3275813
.
[6]
 W. Lv, N. Zhang, J. Zhang, D. Zeng, One-shot multi-
ple ob ject tracking with robust id preservation, IEEE
Transactions on Circuits and Systems for Video Tech-
nology 34 (6) (2024) 4473{4488.
doi:10.1109/TCSVT.
2023.3339609
.
[7]
 Z. Sun, J. Chen, L. Chao, W. Ruan, M. Mukherjee, A
survey of multiple p edestrian tracking based on tracking-
by-detection framework, IEEE Transactions on Circuits
and Systems for Video Technology 31 (5 ) (2021) 1819{
1833.
doi:10.1109/TCSVT.2020.3009717
.
[8]
 H. Zhou, W. Ouyang, J. Cheng, X. Wang, H. Li, Deep
continuous conditional random elds with asymmetric
inter-ob ject constraints for online multi-ob ject tracking,
IEEE Transactions on Circuits and Systems for Video
Technology 29 (4) (2019) 1011{1022.
doi:10.1109/
TCSVT.2018.2825679
.
[9]
 H. No dehi, A. Shahbahrami, Multi-metric re-
identication for online multi-p erson tracking,
IEEE Transactions on Circuits and Syste ms
for Video Technology 32 (1) (2022) 147{159.
doi:10.1109/TCSVT.2021.3059250
.
[10]
 S. You, H. Yao, C. Xu, Multi-ob ject tracking with
spatial-temp oral top ology-based dete ctor, IEEE Trans-
actions on Circuits and Systems for Video Technology
32 (5) (2022) 3023{3035.
doi:10.1109/TCSVT.2021.
3096237
.
14
[11]
 T.-Y. Chung, M. Cho, H. Lee, S. Lee, Ssat: Self-
sup ervised asso ciating network for multiob ject tracking,
IEEE Transactions on Circuits and Systems for Vide o
Technology 32 (11) (2022) 7858{7868.
doi:10.1109/
TCSVT.2022.3186751
.
[12]
 M. Hu, X. Zhu, H. Wang, S. Cao, C. Liu, Q. Song, Std-
former: Spatial-te mp oral motion transformer for multi-
ple ob ject tracking, IEEE Transactions on Circuits and
Systems for Video Technology 33 (11) (2023) 6571{6594.
doi:10.1109/TCSVT.2023.3263884
.
[13]
 K. Shim, J. Byun, K. Ko, J. Hwang, C. Kim, Enhancing
robustness of multi-ob ject trackers with temp oral fea-
ture mix, IEEE Transactions on Circuits and Systems
for Video Technology (2024) 1{1
doi:10.1109/TCSVT.
2024.3403166
.
[14]
 H. W. Kuhn, The hungarian metho d for the assignment
problem, Naval Res earch Logistics Quarterly 2 (1-
2) (1955) 83{97.
arXiv:https://onlinelibrary.
wiley.com/doi/pdf/10.1002/nav.3800020109
,
doi:10.1002/nav.3800020109
.
URL
https://onlinelibrary.wiley.com/doi/abs/
10.1002/nav.3800020109
[15]
 A. Bewley, Z. Ge, L. Ott, F. Ramos, B. Up croft, Sim-
ple online and realtime tracking, in: 2016 IEEE Inter-
national Conference on Image Pro ces sing (ICIP), 2016,
pp. 3464{3468.
doi:10.1109/ICIP.2016.7533003
.
[16]
 N. Wo jke, A. Bewley, D. Paulus, Simple online and
realtime tracking with a deep asso ciation metric, in:
2017 I EEE International Conference on Image Pro c ess-
ing (ICIP), 2017, p. 3645{3649.
doi:10.1109/ICIP.
2017.8296962
.
[17]
 Y. Du, Z. Zhao, Y. Song, Y. Zhao, F. Su, T. Gong,
H. Meng, Strongsort: Make deepsort great again, IEEE
Transactions on Multimedia 25 (2023) 8725{8737.
doi:
10.1109/TMM.2023.3240881
.
[18]
 J. Yin, W. Wang, Q. Meng, R. Yang, J. Shen, A unied
ob ject motion and anity mo del for online multi-ob ject
tracking, in: Pro ceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2020, pp.
6768{6777.
[19]
 J. Seidenschwarz, G. Braso, V. C. Serrano, I. Elezi,
L. Leal-Taixe, Simple cues lead to a strong multi-ob ject
tracke r, in: Pro ce edings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR),
2023, p. 13813{13823.
[20]
 H. Sheng, J. Chen, Y. Zhang, W. Ke, Z. Xiong, J. Yu,
Iterative multiple hyp othesis tracking with tracklet-level
asso ciation, IEEE Transactions on Circuits and Systems
for Video Technology 29 (12) (2019) 3660{3672.
doi:
10.1109/TCSVT.2018.2881123
.
[21]
 H. Sheng, Y. Zhang, J. Chen, Z. Xiong, J. Zhang, Het-
erogeneous asso ciation graph fusion for target asso ci-
ation in multiple ob jec t tracking, I EEE Transactions
on Circuits and Systems for Vide o Technology 29 (11)
(2019) 3269{3280.
doi:10.1109/TCSVT.2018.2882192
.
[22]
 J. Deng, J. Guo, N. Xue, S. Zafeiriou, Arcface: Ad-
ditive angular margin loss for deep face recognition,
in: 2019 IEEE/CVF Confe rence on Computer Vision
and Pattern Recognition (CVPR), 2019, pp. 4685{4694.
doi:10.1109/CVPR.2019.00482
.
[23]
 Y. Guo, L. Zhang, Y. Hu, X. He, J. Gao, Ms-celeb-1m:
A dataset and b enchmark for large-scale face recogni-
tion, in: Computer Vision{ECCV 2016: 14th Europ e an
Conference, Amsterdam, The Netherlands, Octob er 11-
14, 2016, Pro ceedings, Part I I I 14, Springer, 2016, pp.
87{102.
[24]
 Q. Zhao, S. Wang, Real-time face tracking in surveil-
lance videos on chips for valuable face capturing, in:
2020 International Conference on Artic ial Intelligence
and Computer Engineering (ICAICE), 2020, pp. 281{
284.
doi:10.1109/ICAICE51518.2020.00060
.
[25]
 J. Wang, J. Lang, Visual multi-face tracking applied to
council pro c eedings , IEEE Instrumentation & Me asure-
ment Magazine 24 (3) (2021) 78{84.
[26]
 K. Zhang, Z. Zhang, Z. Li, Y. Qiao, Joint face detection
and alignment using multitask cascaded convolutional
networks, IEEE Signal Pro cessing Letters 23 (10) (2016)
1499{1503.
doi:10.1109/LSP.2016.2603342
.
[27]
 W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-
Y. Fu, A. C. Berg, Ssd: Single shot multib ox detec tor,
in: Computer Vision{ECCV 2016: 14th Europ ean Con-
ference, Amsterdam, The Netherlands, Octob er 11{14,
2016, Pro cee dings, Part I 14, Springer, 2016, pp. 21{3 7.
[28]
 J. Dai, Y. Li, K. He, J. Sun, R-fcn: Ob ject detection via
region-based fully convolutional networks, Advances in
neural information pro cessing systems 29 (2016).
[29]
 N. Wo jke, A. Bewley, Deep cosine metric learning for
p erson re-identication, in: 2018 IEEE Winter Con-
ference on Applications of Computer Vision (WACV),
2018, pp. 748{756.
doi:10.1109/WACV.2018.00087
.
[30]
 W. Liu, Y. Wen, Z. Yu, M. Li, B. Ra j, L. Song,
Sphereface: Deep hyp ersphere emb edding for face recog-
nition, in: Pro ceedings of the IEEE conference on com-
puter vision and pattern recognition, 2017, pp. 212{220.
[31]
 Y. Alayary, N. Shoukry, M. A. A. El Ghany, M. A.-
M. Salem, Face masked and unmasked humans detec-
tion and tracking in video surveillance, in: 2022 4th
15
Nove l Intelligent and Leading Emerging Sciences C on-
ference (NILES), 2022, pp. 211{215.
doi:10.1109/
NILES56402.2022.9942375
.
[32]
 T. M. Tran, N. H. Tran, S. T. M. Duong, H. D. Ta, C. D.
Nguyen, T. Bui, S. Q. Truong, Resort: an id-recovery
multi-face tracking metho d for surveillance cameras, in:
2021 16th IEEE International Confe re nc e on Automatic
Face and Gesture Rec ognition (FG 2021), 2021, pp. 01{
08.
doi:10.1109/FG52635.2021.9666941
.
[33]
 D. Marcetic, S. Ribaric, An online multi-fac e tracker for
unconstrained videos, in: 2018 14th International Con-
ference on Signal-Image Technology & Internet-Based
Systems (SITIS), 2018, pp. 159{165.
doi:10.1109/
SITIS.2018.00033
.
[34]
 G. Barque ro, C. Fernandez, I. Hup ont, Long-term face
tracking for crowded video-surveillance scenarios, in:
2020 IEEE International Joint Conference on Biomet-
rics (IJCB), 2020, pp. 1{8.
doi:10.1109/IJCB48548.
2020.9304892
.
[35]
 F. Pernici, M. Bruni, A. Del Bimb o, Self-sup ervised on-
line cumulative learning from video streams, Computer
Vision and Image Understanding 197-198 (2020) 102983.
doi:10.1016/j.cviu.2020.102983
.
[36]
 Y. J. Wong, K. Huang Lee , M.-L. Tham, B.-H. Kwan,
Multi-camera face detection and recognition in uncon-
strained environment, in: 2023 IEEE World AI IoT
Congress (AI IoT), 2023, pp. 0548{0553.
doi:10.1109/
AIIoT58121.2023.10174362
.
[37]
 J. Redmon, S. Divvala, R. Girshick, A. Farhadi, You
only lo ok once: Unied, real-time ob je ct detection, in:
2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016, pp. 779{788.
doi:10.1109/
CVPR.2016.91
.
[38]
 W. Liu, Y. Wen, B. Ra j, R. Singh, A. Weller, Sphereface
revived: Unifying hyp erspherical face recognition, IEEE
Transactions on Patte rn Analysis and Machine Intelli-
gence 45 (2) (2023) 2458{2474.
doi:10.1109/TPAMI.
2022.3159732
.
[39]
 O. Parkhi, A. Vedaldi, A. Zisserman, Deep face recog-
nition, in: BMVC 2015-Pro ceedings of the British Ma-
chine Vision Conference 2015, British Machine Vision
Asso ciation, 2015.
[40]
 F. Schro, D. Kalenichenko, J. Philbin, Facenet: A uni-
ed emb edding for face recognition and clustering, in:
Pro ceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), 2015.
[41]
 Y. Zhong, W. De ng, J. Hu, D. Zhao, X. Li, D. Wen,
Sface: Sigm oid-c ons trained hyp ersphere loss for robust
face recognition, IEEE Transactions on Image Pro-
cessing 30 (2021) 2587{2598.
doi:10.1109/TIP.2020.
3048632
.
[42]
 Z. Weng, H. Zhuang, H. Li, B. Ramalingam, R. E.
Mohan, Z. Lin, Online multi-face tracking with multi-
mo dality cascaded m atching, IEEE Transactions on Cir-
cuits and Systems for Video Technology 33 (6) (2023)
2738{2752.
doi:10.1109/TCSVT.2022.3224699
.
[43]
 J. Hu, L. Shen, G. Sun, Squeeze-and-excitation net-
works, in: Pro ceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2018.
[44]
 Q. Cao, L. Shen, W. Xie, O. M. Parkhi, A. Zisserman,
Vggface2: A dataset for recognising faces across p ose
and age, in: 2018 13th IEEE Inte rnational Conference
on Automatic Face & Gesture Recognition (FG 2018),
2018, pp. 67{74.
doi:10.1109/FG.2018.00020
.
[45]
 K. He, X. Zhang, S. Ren, J. Sun, Deep residual learn-
ing for image recognition, in: Pro ceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2016.
[46]
 Q. Huang, Y. Xiong, A. Rao, J. Wang, D. Lin, Movienet:
A holis tic dataset for m ovie understanding, in: Com-
puter Vision{ECCV 2020 : 16th Europ ean Conference,
Glasgow, UK, August 23{28, 2020, Pro cee dings, Part
IV 16, Springer, 2020, pp. 709{727.
[47]
 Y. Wong, S. Chen, S. Mau, C. Sanderson, B. C. Lovell,
Patch-base d probabilistic image quality assessment for
face selection and improved vide o-based face recognition,
in: IEEE Biometrics Workshop, Computer Vision and
Pattern Recognition (CVPR) Workshops, IEEE, 2011,
pp. 81{88.
[48]
 Z. Wang, L. Zheng, Y. Liu, Y. Li, S. Wang, Towards real-
time multi-ob ject tracking, in: Europ ean co nference on
computer vision, Springe r, 2020, pp. 107{122.
[49]
 Y. Du, J. Wan, Y. Zhao, B. Zhang, Z. Tong, J. Dong,
Giaotracke r: A comprehensive frame work for mcmot
with global information and optimizing strategies in vis-
drone 2021, in: Pro ceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV) Work-
shops, 2021, pp. 2809{2819.
[50]
 A. Dutta, A. Zisserman, The VIA annotation soft-
ware for images, audio and video, in: Pro ceedings of
the 27th ACM International Conference on Multime-
dia, MM '19, ACM, New York, NY , USA, 201 9.
doi:
10.1145/3343031.3350535
.
URL
https://doi.org/10.1145/3343031.3350535
[51]
 P. Dendorfer, H. Rezatoghi, A. Milan, J. Shi, D. Cre-
mers, I. Reid, S. Ro th, K. Schindler, L. Leal-Taixe,
16
Mot20: A b enchmark for multi ob ject tracking in
crowded scenes, arXiv:2003.09003[cs]ArXiv: 2003.09003
(Mar. 2020).
URL
http://arxiv.org/abs/1906.04567
[52]
 J. Deng, W. Dong, R. So cher, L.-J. Li, K. Li, L. Fei-Fei,
Imagenet: A large-scale hierarchical image database, in:
2009 IEEE conference on computer vision and pattern
recognition, Ieee, 2009, pp. 248{255.
[53]
 S. I. Serengil, A. Ozpinar, Lightface: A hybrid
deep face recognition framework, in: 2020 In-
novations in Intelligent Systems and Applica-
tions Conference (ASYU), IEEE, 2020, pp. 23{27.
doi:10.1109/ASYU50717.2020.9259802
.
URL
https://doi.org/10.1109/ASYU50717.2020.
9259802
[54]
 S. I. Serengil, A. Ozpinar, Hyp erextended light-
face: A facial attribute analysis framework, in: 2021
International Conference on Engineering and Emerg-
ing Technologies (ICEET), IEEE, 2021, pp. 1{4.
doi:10.1109/ICEET53442.2021.9659697
.
URL
https://doi.org/10.1109/ICEET53442.2021.
9659697
[55]
 G. B. Huang, M. Mattar, T. Berg, E. Learned-Miller,
Lab eled faces in the wild: A database forstudying face
recognition in unc onstrained environments, in: Work-
shop on faces in'Real-Life'Images: detection, alignment,
and recognition, 2008.
[56]
 L. Wolf, T. Hassner, I. Maoz, Face recognition in uncon-
strained videos with matched background similarity, in:
CVPR 2011, IEEE, 2011, pp. 529{534.
[57]
 D. Yi, Z. Lei, S. Liao, S. Z. Li, Learning face repre-
sentation from scratch, arXiv preprint arXiv:1411.7923
(2014).
[58]
 B. Amos , B. Ludwiczuk, M. Satyanarayanan, Op enface:
A general-purp ose face recognition library with mobile
applications, Tech. rep., CMU-CS-16-118, CMU Scho ol
of Computer Science (2016).
[59]
 H.-W. Ng, S. Winkler, A data-driven approach to clean-
ing large face datasets, in: 2014 IEEE international con-
ference on image pro cessing (ICIP), IEEE, 2014, pp.
343{347.
[60]
 Y. Taigman, M. Ya ng, M. Ranzato, L. Wolf, Deep-
face: Closing the gap to human-level p erformance in
face verication, in: Pro ceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR),
2014.
[61]
 Y. Sun, X. Wang, X. Tang, Deep learning face represen-
tation from predicting 10,000 classes, in: Pro ceedings of
the IEEE Conference on Computer Vis ion and Pattern
Recognition (CVPR), 2014.
[62]
 Leondgarse, Keras insightface,
https://github.com/
leondgarse/Keras_insightface
(2022).
doi:10.
5281/zenodo.6506949
.
[63]
 Davisking, Dlib,
https://github.com/davisking/
dlib
(2022).
[64]
 K. Bernardin, R. Stiefelhagen, Evaluating multiple
ob je ct tracking p erformance: the clear mot metrics,
EURASIP Journal on Image and Video Pro cessing 2008
(2008) 1{10.
[65]
 E. Ristani, F. Solera, R. Zou, R. Cucchiara, C. Tomasi,
Performance measures and a data set for multi-target,
multi-camera tracking, in: Europ ean conference on c om-
puter vision, Springer, 2016, pp. 17{35.
[66]
 J. Luiten, A. Os ep, P. Dendorfer, P. Torr, A. Geiger,
L. Leal-Taixe, B. Leib e, Hota: A higher order metric fo r
evaluating multi-ob ject tracking, International journal
of computer vision 129 (2) (2021) 548{578.
doi:10.
1007/s11263- 020- 01375- 2
.
[67]
 F. Boutros , M. Fang, M. Klemt, B. Fu, N. Damer, Cr-
qa: Face image quality assessment by learning sample
relative classiability, in: Pro ceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2023, pp. 5836{5845.
[68]
 Q. Meng, S. Zhao, Z. Huang, F. Zhou, Magface: A uni-
versal representation for face re cognition and quality as-
sessment, in: Pro c eedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR),
2021, pp. 14225{14234.
[69]
 J. Hernandez-Ortega, J. Fierrez, I. Serna, A. Morales,
Faceqgen: Semi-sup ervised deep learning for face image
quality assessment, in: 2021 16th IEEE International
Conference on Automatic Face and Gesture Recognition
(FG 2021), 2021, pp. 1{8.
doi:10.1109/FG52635.2021.
9667060
.
[70]
 S. Zhang, Y. Gong, J.-B. Huang, J. Lim, J. Wang,
N. Ahuja, M.-H. Yang, Tracking p ersons-of-interest via
adaptive discriminative features, in: Euro p ean Confer-
ence on Computer Vision, 2016, pp. 415{433.
17
